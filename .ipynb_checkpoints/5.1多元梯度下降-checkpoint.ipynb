{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.特征缩放（feature scaling)\n",
    "<font size=5><br>1.0这个特征缩：确保不同的特征的取值在相近的范围内，这样就可以使梯度下降法更快地收敛\n",
    "         <br>首先我们先忽略θ0，因为θ0只是一个校正值，如果我们先看θ1，θ2画轮廓图\n",
    "    <br>然后我们可以看到，两个维度的值相差非常大，因此画出来的轮廓图是瘦长的椭圆，这样做梯度下降，得到最小值，是非常慢的\n",
    "     <br>2.0因此我们可以参照右边的做法，把x1/2000，x2/5，这样轮廓图就可以变成更标准一点的圆。所以通过特征值缩放消耗掉这些值的范围。这样0<=x1，x2<=1\n",
    "    \n",
    "</font>\n",
    "\n",
    "\n",
    "![title](25.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<font size=5><br>2.0更加一般的特征缩放是要让 -1<= xi<=1 ，而x0=1是一定的，这个取值范围并不是死板的。\n",
    "        <br>如果你有两个特征 0<=x1<=3  , -2<=x2<=0.5 也是可以的，因为这两个特征也十分接近了，而如果\n",
    "        -100<= x3 <=100 就不行了，同样 -0.0001<=x4<=0.0001 也不行。\n",
    "        <br>参考的特征范围可以如下\n",
    "    \n",
    "</font>\n",
    "\n",
    "\n",
    "![title](26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.均值归一化(Mean Normalization)\n",
    "\n",
    "<font size=5><br>1.0让特征xi被xi-ui代替。这个ui可取为所有特征xj[i]的平均值。例如房子大小的平均值为1000，最大值为2000，可得如下的内容。\n",
    "          <br>更一般的归一化是，然（xi-ui）/s1。ui取为平均值即可，s1为样本标准差，其实取最大值减去最小值即可\n",
    "       \n",
    "</font>\n",
    "\n",
    "\n",
    "![title](27.png)\n",
    "\n",
    "\n",
    "\n",
    "<font size=5><br>以上两种方法所得值都不是非常精确，只不过是为了让梯度下降能够顺利进行罢了\n",
    "       \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.如何调试梯度下降和选择学习效率α\n",
    "\n",
    "![title](28.png)\n",
    "\n",
    "<font size=5>\n",
    "         <center>判断梯度下降是否正常工作，以及收敛点的选择</center>\n",
    "         <br>\n",
    "         <br>1.0第一个问题是我们如何确定梯度下降算法是否正确工作\n",
    "         <br>2.0下图的横坐标表示梯度下降的迭代步数，而之前的横坐标是表示θ值，从图中就可以看出，迭代的步数越多，代价函数j(θ)的值会越来越小，然后就会慢慢收敛。\n",
    "         <br>3.0因此如果梯度下降运行的正确的，那么每一次迭代以后，j(θ)的值都会减小，呈现出以下图样\n",
    "         <br>4.0那么每次迭代下降多少可以被称为已经收敛呢，也就是j(θ）达到最小值呢，这个值可能是10[-3]每一步迭代。但是这个数字有多小还是很难选择，所以一般还是通过看图来找出收敛的部分，同时看图也能判断出梯度下降是否正确运行\n",
    "       \n",
    "</font>\n",
    "\n",
    "![title](29.png)\n",
    "\n",
    "\n",
    "\n",
    "<font size=5>\n",
    "         <center>α的选择</center>\n",
    "         <br>\n",
    "         <br>1.0如果运行梯度下降算法，看到如下的图样就表明，我们需要选择一个更小的学习效率α\n",
    "         <br>2.0只要我们的α足够小，那么每一步迭代，都可以看到代价函数j(θ)在减小，但不意味着这个α越小越好，太小的话，梯度下降算法就会收敛的很慢\n",
    "</font>    \n",
    "\n",
    "\n",
    "![title](30.png)\n",
    "\n",
    "<font size=5>\n",
    "         <br>3.0因此选择α，可以按照一下顺序，然后尝试选出尽可能打的α\n",
    "   \n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    " ![title](31.png)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
