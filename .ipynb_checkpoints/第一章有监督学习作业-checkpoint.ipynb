{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.warmUpExercise\n",
    "<font size=5>首先第一步练习，要返回一个5x5的单位矩阵，作业描述如下，答案也在其中\n",
    "</font>\n",
    "\n",
    "\n",
    "![title](84.png)\n",
    "\n",
    "# 2.plotData(x, y)\n",
    "<font size=5>你是一家快餐店的ceo，打算在别的地方在开一家连锁店，这里的数据集里有每个城市的人口，以及收益的数据集，负数代表损失。首先我们应该把数据可视化，答案如下.首先load数据，然后在plot函数里画，结果如下，但应该是没有图例和横线的\n",
    "</font>\n",
    "\n",
    "\n",
    "![title](85.png)\n",
    "\n",
    "![title](86.png)\n",
    "\n",
    "![title](87.png)\n",
    "\n",
    "# 3.Batch gradient descent\n",
    "<font size=5>以上数据可视化后，根据说明书的定义，模型为 h（x）=θ0+θ1x1。下一步应该使用梯度下降求解最小化代价函数j(θ）,因此使用批量梯度下降求θ，代价函数如下.\n",
    "</font>\n",
    "\n",
    "![title](88.png)\n",
    "\n",
    "<font size=5><br>1.0先定义一个设计矩阵X，因为数据只有一个特征，因此设计矩阵X定义为，第一列为1这是根据模型设的，第二列根据存入的是特征，代码如下，其中data为数据集，m为数据集大小，ones为全为1的列，然后分别初始化θ0，θ1，最大迭代数，学习效率α\n",
    "</font>\n",
    "\n",
    "![title](90.png)\n",
    "\n",
    "<font size=5><br>2.0计算代价函数j(θ)，代码如下，因为矩阵θ是一个2X1，矩阵X是为mX2 所以求每一行的h(θ)就是让设计矩阵X*θ，全部如下，当θ0=0,θ1=0时，可以算出j(θ)=32.073\n",
    "</font>\n",
    "\n",
    "![title](91.png)\n",
    "\n",
    "<font size=5><br>3.0接下来要开始使用梯度下降算法进行迭代求最小值了，首先更新算法如下，求完偏导后的结果。此时我们要把h-y的部分看成是矩阵，theta=[θ0;θ1]。在让设计矩阵X转置乘以(h-y向量即可)，答案如下。算出的tehta矩阵在传给另外一个函数算代价函数j存在矩阵中就可以了，得到收敛的theta后就可也画出如下图的线\n",
    "</font>\n",
    "\n",
    "![title](3.PNG)\n",
    "\n",
    "![title](92.PNG)\n",
    "\n",
    "![title](93.PNG)\n",
    "# 4.Visualizing J(θ)\n",
    "<font size=5><br>4.0把求梯度下降时记录下来的j(θ)可视化\n",
    "</font>\n",
    "\n",
    "# 选做 5.Linear regression with multiple variables\n",
    "\n",
    "<font size=5><br>1.0首先我们观察数据集的值，他们的值相差很大，因此我们必须做归一化处理，这样求梯度下降时，能更快的收敛。以下mu存储平均值，sigma存储标准差。这些值以后与预测新的数据时，可以用来归一化数据。做法如下\n",
    "</font>\n",
    "\n",
    "![title](94.PNG)\n",
    "\n",
    "\n",
    "<font size=5><br>2.0数据归一化以后的结果如下\n",
    "</font>\n",
    "\n",
    "![title](95.PNG)\n",
    "\n",
    "<font size=5><br>3.0计算j(θ)的值，这个其实和单变量的没有区别，方法还是一样的\n",
    "</font>\n",
    "\n",
    "![title](91.png)\n",
    "\n",
    "<font size=5><br>4.0接下来是梯度下降算法，多一维实际上也一样\n",
    "</font>\n",
    "\n",
    "![title](92.PNG)\n",
    "\n",
    "<font size=5><br>5.0画出j(θ)随着θ变化的的迭代图，代码如下，其中numel()作用是返回矩阵有多少个数，画图代码如下，一开始画出1：400的横坐标，表示迭代数，然后第二个参数放入历史矩阵，每迭代一步都对应一个数，因此可以画出下图，j的值越来越小，随着迭代步数，慢慢的收敛，可以得出我们得到了全局最优解，此时对应的α=0.1，当α=0.3，α=0.9时，会更快的收敛，如下图\n",
    "</font>\n",
    "\n",
    "\n",
    "![title](97.PNG)\n",
    "\n",
    "![title](100.PNG)\n",
    "\n",
    "\n",
    "<font size=5><br>6.0预测房子大小为1650，房间数为3的房价，代码如下，用已经取得的平均值和标准差去归一化这些值。预测结果如下\n",
    "</font>\n",
    "\n",
    "![title](101.PNG)\n",
    "\n",
    "![title](102.PNG)\n",
    "<font size=5><br>7.0 Normal Equations正态方程求解θ，不再需要迭代和归一化数值，公式如下，要注意设计矩阵X包括了全为1的一列\n",
    "</font>\n",
    "\n",
    "![title](103.PNG)\n",
    "\n",
    "![title](104.PNG)\n",
    "\n",
    "<font size=5><br>8.0可以看到尽管θ的值与梯度下降所得不一样，但是对于同一条数据集的预测结果是一样的。主要是因为，正态方程求解不需要归一化，所以求出的参数值，更小\n",
    "</font>\n",
    "\n",
    "![title](105.PNG)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
