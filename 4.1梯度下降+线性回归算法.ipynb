{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.用梯度下降法求解最小化代价函数\n",
    "![title](2.PNG)\n",
    "<font size=5>\n",
    "    <br>上面的算法我们已经很熟悉了，为了写出梯度下降的代码，我们必须弄清楚微分项\n",
    "    <br>以下就是按照求偏导的展开，实际上每一个微分，都是代价函数j的斜率\n",
    "</font>\n",
    "\n",
    "![title](3.jpg)\n",
    "\n",
    "\n",
    "\n",
    "![title](7.png)\n",
    "<font size=5>\n",
    "    <br>随后我们便得到了梯度下降的算法，一直重复如下操作，直到收敛为止，也就是θ0，θ1为0为止\n",
    "</font>\n",
    "\n",
    "![title](3.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.梯度下降的工作\n",
    "<font size=5>\n",
    "    <br>1.0 在线性回归中，我们得到的代价函数j，不存在局部最优解，总是能得到一个全局最优解。因为线性回归中的代价函数，总是一种称为convex的凸函数（国内的课本称为凹函数），图像如下。\n",
    "</font>\n",
    "\n",
    "![title](4.PNG)\n",
    "\n",
    "<font size=5>\n",
    "    <br>2.0在开始工作时，如果设θ0=900，θ1=-0.1。随着算法的迭代，可以看到代价函数j的值越来越接近最小花费，最后到达最小化花费处，h函数就能很好的拟合这些点了\n",
    "</font>\n",
    "\n",
    "![title](5.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.批量梯度下降\n",
    "<font size=5>\n",
    "    <br>在学术界，我们刚刚的算法实际上被称为批量梯度下降，因为可以看到每次算偏导数的时候，我们需要把整个数据集里的数据都算一次欧氏距离。因此称之为批量。，在别的算法里也有不计算整个数据集的\n",
    "</font>\n",
    "\n",
    "![title](6.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
