{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.多元线性回归\n",
    "<font size=5><br>1.0先引入一个训练集如下\n",
    "         <br> n:特征数\n",
    "         <br>m:训练集条数\n",
    "         <br>x[i]:第i条训练数据\n",
    "         <br>xj[i]:第i条训练数据的第j个参数\n",
    "\n",
    "</font>\n",
    "\n",
    "\n",
    "\n",
    "![title](121.png)\n",
    "\n",
    "<font size=5><br>2.0由于有多个特征，我们先前的假设函数h就不再使用了，而是变成下面这个\n",
    "        \n",
    "\n",
    "</font>\n",
    "\n",
    "![title](21.png)\n",
    "\n",
    "<font size=5><br>3.0由于上面那个公式过于复杂，我们来简化一下\n",
    "         <br>我们为了统一公式，而在θ0后定义一个x0，并未他赋值为1，于是我们相当于在整个训练集中引入一维，\n",
    "    因为可以得x0[i]=1\n",
    "         <br>然后我们将参数和训练集向量化，写成向量(vector)的形式，于是这个假设函数h就可以写成向量相乘的形式，时间上是两个向量的内积\n",
    "</font>\n",
    "\n",
    "![title](22.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.多元梯度下降\n",
    "\n",
    "<font size=5><br>1.0我们不要把这些θ认为是n+1个单独的参数，而是要把它想成n+1维的向量称为向量θ\n",
    "         <br>同时也不要把代价函数j（θ0，θ1......θn）看成n+1个变量的函数，而是看成带有n+1维向量的函数\n",
    "         <br>j(θ(n+1 - dimensional vector))函数，于是我们就可以得到最新的梯度下降算法(Gradient descent)\n",
    "\n",
    "</font>\n",
    "\n",
    "![title](23.png)\n",
    "\n",
    "\n",
    "<font size=5><br>2.0左边是一个特征的梯度下降，我们可以知道现在的x1(i)应该写为x1[i],第i条的第一个特征\n",
    "         <br>右边是多个特征的梯度下降，通过对比可知，他们其实算法相同，因为x0[i]被定义为1\n",
    "</font>\n",
    "\n",
    "\n",
    "![title](24.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
